{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cart_Pole_REINFORCE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMa4r37Og9+AafJo9zDM4IR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoumyadeepB/Reinforcement-Learning/blob/master/Cart_Pole_REINFORCE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAbXJmXHSVtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwrEqYW94QBk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9076691a-b959-4285-dcf7-9229b65c367f"
      },
      "source": [
        "env = gym.make('CartPole-v1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0Eq_TYdSwYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' For a  Neural Network Model:\n",
        "def policy_model(env):\n",
        "    n_states = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.n \n",
        "    model = Sequential()\n",
        "    model.add(Dense(8, input_dim=n_states, activation='relu'))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(n_actions, activation='sigmoid'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001))\n",
        "\n",
        "    return model'''\n",
        "\n",
        "\n",
        "def policy(env, state, theta):\n",
        "    \"\"\" TODO: return probabilities for actions under softmax action selection \"\"\"\n",
        "    p = np.dot(state,theta).flatten()\n",
        "    \n",
        "    policy = 1.0 / (1.0 + np.exp(-p)) #Sigmoid Function\n",
        "    #print(\"Sigmoid:\",policy)\n",
        "    total = np.sum(policy)\n",
        "    policy = policy/total\n",
        "    #print(policy)\n",
        "    \n",
        "    return policy  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8m8JzaVDwHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discounted_rewards(rewards):\n",
        "  gamma = 0.1\n",
        "  G = np.zeros_like(rewards)\n",
        "  future_rewards = 0\n",
        "  for i in reversed(range(0,len(rewards))):\n",
        "    future_rewards = gamma*future_rewards + rewards[i]\n",
        "    G[i] = future_rewards    # G_t = R_(t+1) + Î³ * G_(t+1) \n",
        "\n",
        "  return G"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUcIXVbYWUox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient(state,theta):\n",
        "  # Using Finite differences\n",
        "  h = 0.001\n",
        "  theta1 = theta + h\n",
        " \n",
        "  f0 = np.log( np.dot(state, theta ) )\n",
        "  f1 = np.log( np.dot(state, theta1) )\n",
        "  #print(\"diff: \",f1-f0)\n",
        "\n",
        "  grad = (f1 - f0)/h\n",
        "\n",
        "  return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U8waqHlS1kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_episode(env, theta, display=False):\n",
        "    \"\"\" Generates one episode and returns the list of states, the list of rewards and the list of actions of that episode \"\"\"\n",
        "    state = env.reset()\n",
        "    states = [state]\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    for t in range(500):\n",
        "        if display:\n",
        "            env.render()\n",
        "        p = policy(env, state, theta)\n",
        "        #print(\"Policy: \",p)\n",
        "        action = np.random.choice(len(p), p=p)\n",
        "\n",
        "        state, reward, done, info = env.step(action)\n",
        "        rewards.append(reward)\n",
        "        actions.append(action)\n",
        "        if done:\n",
        "            break\n",
        "        states.append(state)\n",
        "\n",
        "    return states, rewards, actions\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b-BPY99S4rA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def REINFORCE(env):\n",
        "    theta = np.random.rand(4, 2)  # policy parameters\n",
        "    alpha = 0.01\n",
        "    low = abs(env.observation_space.low)\n",
        "\n",
        "    for e in range(10000):\n",
        "        if e % 300 == 0:\n",
        "            states, rewards, actions = generate_episode(env, theta, False)  # display the policy every 300 episodes\n",
        "        else:\n",
        "            states, rewards, actions = generate_episode(env, theta, False)\n",
        "\n",
        "        T = len(states)\n",
        "        print(\"episode: \" + str(e) + \" length: \" + str(T))\n",
        "\n",
        "        v = discounted_rewards(rewards)\n",
        "        \n",
        "        # TODO: keep track of previous 100 episode lengths and compute mean\n",
        "\n",
        "        # TODO: implement the reinforce algorithm to improve the policy weights\n",
        "        for t in range(T):\n",
        "          state = states[t] + low\n",
        "          \n",
        "          print(state)          \n",
        "          #print(\"Vt: \",v[t])\n",
        "          grad = gradient(state,theta)\n",
        "          #print(grad)\n",
        "\n",
        "          theta +=  alpha * grad * v[t]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPIb4OK_XFzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "REINFORCE(env)\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9gjqIrgH6Pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state= np.array([-0.03470816 ,-0.00487081 , 0.01776325 , 0.0267998 ])\n",
        "theta = np.random.rand(4, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sA5VX__UEAb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "84576f08-6529-4936-d344-3ff7963b8032"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Cart_Pole_REINFORCE.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1SrLmy2AzHyBAW-vd8bkpJGAhtFwCrLjH\n",
        "\"\"\"\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tqdm import tqdm\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "EPISODES = 100000\n",
        "gamma = 0.1\n",
        "theta = np.random.rand(4, 2)  # policy parameters\n",
        "''' For a  Neural Network Model:\n",
        "def policy_model(env):\n",
        "    n_states = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.n \n",
        "    model = Sequential()\n",
        "    model.add(Dense(8, input_dim=n_states, activation='relu'))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(n_actions, activation='sigmoid'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001))\n",
        "    return model'''\n",
        "\n",
        "def policy(env, state, theta):\n",
        "    \"\"\" TODO: return probabilities for actions under softmax action selection \"\"\"\n",
        "    h = np.matmul(state, theta).flatten() # Action preference function\n",
        "    p = np.exp(h) \n",
        "    total = np.sum(p)\n",
        "    policy = p/total\n",
        "    return policy\n",
        "\n",
        "def discounted_rewards(rewards):\n",
        "    G = np.zeros_like(rewards)\n",
        "    future_rewards = 0\n",
        "    for i in reversed(range(0, len(rewards))):\n",
        "        future_rewards = gamma*future_rewards + rewards[i]\n",
        "        G[i] = future_rewards                # G_t = R_(t+1) + Î³ * G_(t+1)\n",
        "    return G\n",
        "\n",
        "def gradient(state, theta, action):\n",
        "    delta = 0.001\n",
        "    theta1 = np.copy(theta)\n",
        "    theta1[:,action] += delta\n",
        "    num = np.dot(state, theta[:,action])\n",
        "    num = np.exp(num)\n",
        "    den = np.exp(np.dot(state, theta[:, 0])) + np.exp(np.dot(state, theta[:, 1]))\n",
        "    f0 = np.log(num/den)\n",
        "    num = np.dot(state, theta1[:,action])\n",
        "    num = np.exp(num)\n",
        "    den = np.exp(np.dot(state, theta1[:, 0])) + np.exp(np.dot(state, theta1[:, 1]))\n",
        "    f1 = np.log(num/den)\n",
        "    gradient = (f1 - f0)/delta\n",
        "    return gradient\n",
        "# def gradient(state, theta):\n",
        "#     h = np.matmul(state, theta)\n",
        "#     p = np.exp(h)\n",
        "#     prob = p / np.sum(p)\n",
        "#     grad = state - (prob[0] * state + prob[1] * state)  # Check David Silver's slide PolicyGradient : Slide 17\n",
        "    \n",
        "#     return grad\n",
        "\n",
        "\n",
        "def generate_episode(env, theta, display=False):\n",
        "    \"\"\" Generates one episode and returns the list of states, the list of rewards and the list of actions of that episode \"\"\"\n",
        "    state = env.reset()\n",
        "    states = [state]\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    for t in range(500):\n",
        "        if display:\n",
        "            env.render()\n",
        "        p = policy(env, state, theta)\n",
        "        #action = np.random.choice(len(p), p=p)\n",
        "        action = np.argmax(p)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        rewards.append(reward)\n",
        "        actions.append(action)\n",
        "        if done:\n",
        "            break\n",
        "        states.append(state)\n",
        "\n",
        "    return states, rewards, actions\n",
        "\n",
        "\n",
        "def REINFORCE(env):\n",
        "    theta = np.random.rand(4, 2)  # policy parameters\n",
        "    theta=theta\n",
        "    alpha = 0.01\n",
        "    episode_lengths = []\n",
        "    length_per_hundred = []\n",
        "    reward_sum = []\n",
        "    reward_per_hundred = []\n",
        "    e = 0\n",
        "\n",
        "    while  e < EPISODES:\n",
        "        if e % 100 == 0:\n",
        "            length_per_hundred.append(np.mean(episode_lengths))\n",
        "            reward_per_hundred.append(reward_sum)\n",
        "            episode_lengths = []\n",
        "            reward_sum = []\n",
        "            states, rewards, actions = generate_episode(env, theta, False)  # display the policy every 300 episodes\n",
        "        else:\n",
        "            states, rewards, actions = generate_episode(env, theta, False)\n",
        "\n",
        "        T = len(states)\n",
        "\n",
        "        if e%1000==0:\n",
        "          print(\"episode: \" + str(e) + \" length: \" + str(T))\n",
        "\n",
        "        v = discounted_rewards(rewards)\n",
        "\n",
        "        # TODO: keep track of previous 100 episode lengths and compute mean\n",
        "        episode_lengths.append(T)\n",
        "        reward_sum.append(np.sum(rewards))\n",
        "\n",
        "        # TODO: implement the reinforce algorithm to improve the policy weights\n",
        "        for t in range(T-1):\n",
        "            state = states[t]\n",
        "            a = actions[t]\n",
        "            grad = gradient(state, theta,a)            \n",
        "            theta += alpha * (gamma ** t) * grad * v[t]\n",
        "\n",
        "        e+=1\n",
        "        if (episode_lengths[-1]<=495 and e>= 10000):            \n",
        "          print(\"Restarting...\")\n",
        "          env.reset()\n",
        "          e = 0\n",
        "          theta = np.random.rand(4, 2)  # policy parameters\n",
        "          episode_lengths = []\n",
        "          length_per_hundred = []\n",
        "\n",
        "    plt.plot(range(len(length_per_hundred)), length_per_hundred,label=\"Episode Length\")    \n",
        "    plt.xlabel(\"Every hundreth episode\")\n",
        "    plt.ylabel(\"Average episode length/reward\")\n",
        "    plt.title(\"Gamma = {}\".format(gamma))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    #plt.plot(range(len(reward_per_hundred)), reward_per_hundred, label=\"Reward\")\n",
        "    #plt.show()\n",
        "\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "gamma = 0.9\n",
        "REINFORCE(env)\n",
        "env.close()\n",
        "\n",
        "# for i in range(1,10):\n",
        "#   gamma = (i/10.0)\n",
        "#   REINFORCE(env)\n",
        "#   env.close()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode: 0 length: 20\n",
            "episode: 1000 length: 12\n",
            "episode: 2000 length: 18\n",
            "episode: 3000 length: 17\n",
            "episode: 4000 length: 15\n",
            "episode: 5000 length: 17\n",
            "episode: 6000 length: 18\n",
            "episode: 7000 length: 26\n",
            "episode: 8000 length: 13\n",
            "episode: 9000 length: 16\n",
            "Restarting...\n",
            "episode: 0 length: 10\n",
            "episode: 1000 length: 8\n",
            "episode: 2000 length: 10\n",
            "episode: 3000 length: 10\n",
            "episode: 4000 length: 10\n",
            "episode: 5000 length: 10\n",
            "episode: 6000 length: 10\n",
            "episode: 7000 length: 10\n",
            "episode: 8000 length: 9\n",
            "episode: 9000 length: 9\n",
            "Restarting...\n",
            "episode: 0 length: 8\n",
            "episode: 1000 length: 10\n",
            "episode: 2000 length: 9\n",
            "episode: 3000 length: 10\n",
            "episode: 4000 length: 9\n",
            "episode: 5000 length: 10\n",
            "episode: 6000 length: 10\n",
            "episode: 7000 length: 10\n",
            "episode: 8000 length: 9\n",
            "episode: 9000 length: 10\n",
            "Restarting...\n",
            "episode: 0 length: 9\n",
            "episode: 1000 length: 9\n",
            "episode: 2000 length: 8\n",
            "episode: 3000 length: 10\n",
            "episode: 4000 length: 8\n",
            "episode: 5000 length: 8\n",
            "episode: 6000 length: 8\n",
            "episode: 7000 length: 10\n",
            "episode: 8000 length: 9\n",
            "episode: 9000 length: 10\n",
            "Restarting...\n",
            "episode: 0 length: 84\n",
            "episode: 1000 length: 102\n",
            "episode: 2000 length: 141\n",
            "episode: 3000 length: 136\n",
            "episode: 4000 length: 176\n",
            "episode: 5000 length: 164\n",
            "episode: 6000 length: 130\n",
            "episode: 7000 length: 130\n",
            "episode: 8000 length: 105\n",
            "episode: 9000 length: 175\n",
            "Restarting...\n",
            "episode: 0 length: 28\n",
            "episode: 1000 length: 27\n",
            "episode: 2000 length: 22\n",
            "episode: 3000 length: 51\n",
            "episode: 4000 length: 33\n",
            "episode: 5000 length: 35\n",
            "episode: 6000 length: 51\n",
            "episode: 7000 length: 26\n",
            "episode: 8000 length: 45\n",
            "episode: 9000 length: 64\n",
            "Restarting...\n",
            "episode: 0 length: 9\n",
            "episode: 1000 length: 10\n",
            "episode: 2000 length: 10\n",
            "episode: 3000 length: 9\n",
            "episode: 4000 length: 8\n",
            "episode: 5000 length: 10\n",
            "episode: 6000 length: 10\n",
            "episode: 7000 length: 8\n",
            "episode: 8000 length: 10\n",
            "episode: 9000 length: 10\n",
            "Restarting...\n",
            "episode: 0 length: 8\n",
            "episode: 1000 length: 9\n",
            "episode: 2000 length: 10\n",
            "episode: 3000 length: 9\n",
            "episode: 4000 length: 8\n",
            "episode: 5000 length: 10\n",
            "episode: 6000 length: 10\n",
            "episode: 7000 length: 9\n",
            "episode: 8000 length: 10\n",
            "episode: 9000 length: 9\n",
            "Restarting...\n",
            "episode: 0 length: 500\n",
            "episode: 1000 length: 500\n",
            "episode: 2000 length: 500\n",
            "episode: 3000 length: 500\n",
            "episode: 4000 length: 500\n",
            "episode: 5000 length: 500\n",
            "episode: 6000 length: 500\n",
            "episode: 7000 length: 500\n",
            "episode: 8000 length: 500\n",
            "episode: 9000 length: 500\n",
            "episode: 10000 length: 500\n",
            "episode: 11000 length: 500\n",
            "episode: 12000 length: 500\n",
            "episode: 13000 length: 500\n",
            "episode: 14000 length: 500\n",
            "episode: 15000 length: 500\n",
            "episode: 16000 length: 500\n",
            "episode: 17000 length: 500\n",
            "episode: 18000 length: 500\n",
            "episode: 19000 length: 500\n",
            "episode: 20000 length: 500\n",
            "episode: 21000 length: 500\n",
            "episode: 22000 length: 500\n",
            "episode: 23000 length: 500\n",
            "episode: 24000 length: 500\n",
            "episode: 25000 length: 500\n",
            "episode: 26000 length: 500\n",
            "episode: 27000 length: 500\n",
            "episode: 28000 length: 500\n",
            "episode: 29000 length: 500\n",
            "episode: 30000 length: 500\n",
            "episode: 31000 length: 500\n",
            "episode: 32000 length: 500\n",
            "episode: 33000 length: 500\n",
            "episode: 34000 length: 500\n",
            "episode: 35000 length: 500\n",
            "episode: 36000 length: 500\n",
            "episode: 37000 length: 500\n",
            "episode: 38000 length: 500\n",
            "episode: 39000 length: 500\n",
            "episode: 40000 length: 500\n",
            "episode: 41000 length: 500\n",
            "episode: 42000 length: 500\n",
            "episode: 43000 length: 500\n",
            "episode: 44000 length: 500\n",
            "episode: 45000 length: 500\n",
            "episode: 46000 length: 500\n",
            "episode: 47000 length: 500\n",
            "episode: 48000 length: 500\n",
            "episode: 49000 length: 500\n",
            "episode: 50000 length: 500\n",
            "episode: 51000 length: 500\n",
            "episode: 52000 length: 500\n",
            "episode: 53000 length: 500\n",
            "episode: 54000 length: 500\n",
            "episode: 55000 length: 500\n",
            "episode: 56000 length: 500\n",
            "episode: 57000 length: 500\n",
            "episode: 58000 length: 500\n",
            "episode: 59000 length: 500\n",
            "episode: 60000 length: 500\n",
            "episode: 61000 length: 500\n",
            "episode: 62000 length: 500\n",
            "episode: 63000 length: 500\n",
            "episode: 64000 length: 500\n",
            "episode: 65000 length: 500\n",
            "episode: 66000 length: 500\n",
            "episode: 67000 length: 500\n",
            "episode: 68000 length: 500\n",
            "episode: 69000 length: 500\n",
            "episode: 70000 length: 500\n",
            "episode: 71000 length: 500\n",
            "episode: 72000 length: 500\n",
            "episode: 73000 length: 500\n",
            "episode: 74000 length: 500\n",
            "episode: 75000 length: 500\n",
            "episode: 76000 length: 500\n",
            "episode: 77000 length: 500\n",
            "episode: 78000 length: 500\n",
            "episode: 79000 length: 500\n",
            "episode: 80000 length: 500\n",
            "episode: 81000 length: 500\n",
            "episode: 82000 length: 500\n",
            "episode: 83000 length: 500\n",
            "episode: 84000 length: 500\n",
            "episode: 85000 length: 500\n",
            "episode: 86000 length: 500\n",
            "episode: 87000 length: 500\n",
            "episode: 88000 length: 500\n",
            "episode: 89000 length: 500\n",
            "episode: 90000 length: 500\n",
            "episode: 91000 length: 500\n",
            "episode: 92000 length: 500\n",
            "episode: 93000 length: 500\n",
            "episode: 94000 length: 500\n",
            "episode: 95000 length: 500\n",
            "episode: 96000 length: 500\n",
            "episode: 97000 length: 500\n",
            "episode: 98000 length: 500\n",
            "episode: 99000 length: 500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV3j4_TxlDb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient(state, theta, action):\n",
        "    # Using Finite differences\n",
        "    h = np.dot(state, theta)\n",
        "    p = np.exp(h)\n",
        "    prob = p / np.sum(p)\n",
        "    grad = state - (prob[0] * state + prob[1] * state)  # Check David Silver's slide PolicyGradient : Slide 17\n",
        "\n",
        "    return grad[action]\n",
        "\n",
        "\n",
        "    def gradient(state, theta):\n",
        "    # Using Finite differences\n",
        "    h = 0.0001\n",
        "    theta1 = theta + h\n",
        "    f0 = np.log(np.exp(np.dot(state, theta)))\n",
        "    f1 = np.log(np.exp(np.dot(state, theta1)))\n",
        "    grad = (f1 - f0)/h\n",
        "\n",
        "    return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_CSmvD0ZAvU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fc65570f-a07a-4f8d-cd64-97d2e13b8d10"
      },
      "source": [
        "np.ones((4,2))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1.],\n",
              "       [1., 1.],\n",
              "       [1., 1.],\n",
              "       [1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5kveHSRZCJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
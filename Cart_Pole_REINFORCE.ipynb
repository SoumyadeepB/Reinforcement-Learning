{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cart_Pole_REINFORCE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEQfuTmjSlSAMvq9EjnI5O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoumyadeepB/Reinforcement-Learning/blob/master/Cart_Pole_REINFORCE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAbXJmXHSVtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwrEqYW94QBk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9076691a-b959-4285-dcf7-9229b65c367f"
      },
      "source": [
        "env = gym.make('CartPole-v1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0Eq_TYdSwYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' For a  Neural Network Model:\n",
        "def policy_model(env):\n",
        "    n_states = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.n \n",
        "    model = Sequential()\n",
        "    model.add(Dense(8, input_dim=n_states, activation='relu'))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(n_actions, activation='sigmoid'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001))\n",
        "\n",
        "    return model'''\n",
        "\n",
        "\n",
        "def policy(env, state, theta):\n",
        "    \"\"\" TODO: return probabilities for actions under softmax action selection \"\"\"\n",
        "    p = np.dot(state,theta).flatten()\n",
        "    \n",
        "    policy = 1.0 / (1.0 + np.exp(-p)) #Sigmoid Function\n",
        "    #print(\"Sigmoid:\",policy)\n",
        "    total = np.sum(policy)\n",
        "    policy = policy/total\n",
        "    #print(policy)\n",
        "    \n",
        "    return policy  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8m8JzaVDwHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discounted_rewards(rewards):\n",
        "  gamma = 0.1\n",
        "  G = np.zeros_like(rewards)\n",
        "  future_rewards = 0\n",
        "  for i in reversed(range(0,len(rewards))):\n",
        "    future_rewards = gamma*future_rewards + rewards[i]\n",
        "    G[i] = future_rewards    # G_t = R_(t+1) + γ * G_(t+1) \n",
        "\n",
        "  return G"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUcIXVbYWUox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient(state,theta):\n",
        "  # Using Finite differences\n",
        "  h = 0.001\n",
        "  theta1 = theta + h\n",
        " \n",
        "  f0 = np.log( np.dot(state, theta ) )\n",
        "  f1 = np.log( np.dot(state, theta1) )\n",
        "  #print(\"diff: \",f1-f0)\n",
        "\n",
        "  grad = (f1 - f0)/h\n",
        "\n",
        "  return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U8waqHlS1kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_episode(env, theta, display=False):\n",
        "    \"\"\" Generates one episode and returns the list of states, the list of rewards and the list of actions of that episode \"\"\"\n",
        "    state = env.reset()\n",
        "    states = [state]\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    for t in range(500):\n",
        "        if display:\n",
        "            env.render()\n",
        "        p = policy(env, state, theta)\n",
        "        #print(\"Policy: \",p)\n",
        "        action = np.random.choice(len(p), p=p)\n",
        "\n",
        "        state, reward, done, info = env.step(action)\n",
        "        rewards.append(reward)\n",
        "        actions.append(action)\n",
        "        if done:\n",
        "            break\n",
        "        states.append(state)\n",
        "\n",
        "    return states, rewards, actions\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b-BPY99S4rA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def REINFORCE(env):\n",
        "    theta = np.random.rand(4, 2)  # policy parameters\n",
        "    alpha = 0.01\n",
        "    low = abs(env.observation_space.low)\n",
        "\n",
        "    for e in range(10000):\n",
        "        if e % 300 == 0:\n",
        "            states, rewards, actions = generate_episode(env, theta, False)  # display the policy every 300 episodes\n",
        "        else:\n",
        "            states, rewards, actions = generate_episode(env, theta, False)\n",
        "\n",
        "        T = len(states)\n",
        "        print(\"episode: \" + str(e) + \" length: \" + str(T))\n",
        "\n",
        "        v = discounted_rewards(rewards)\n",
        "        \n",
        "        # TODO: keep track of previous 100 episode lengths and compute mean\n",
        "\n",
        "        # TODO: implement the reinforce algorithm to improve the policy weights\n",
        "        for t in range(T):\n",
        "          state = states[t] + low\n",
        "          \n",
        "          print(state)          \n",
        "          #print(\"Vt: \",v[t])\n",
        "          grad = gradient(state,theta)\n",
        "          #print(grad)\n",
        "\n",
        "          theta +=  alpha * grad * v[t]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPIb4OK_XFzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "REINFORCE(env)\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9gjqIrgH6Pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state= np.array([-0.03470816 ,-0.00487081 , 0.01776325 , 0.0267998 ])\n",
        "theta = np.random.rand(4, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sA5VX__UEAb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "efc68240-f404-4495-a3be-68950ee02f03"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Cart_Pole_REINFORCE.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1SrLmy2AzHyBAW-vd8bkpJGAhtFwCrLjH\n",
        "\"\"\"\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tqdm import tqdm\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "EPISODES = 100000\n",
        "gamma = 0.1\n",
        "\n",
        "''' For a  Neural Network Model:\n",
        "def policy_model(env):\n",
        "    n_states = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.n \n",
        "    model = Sequential()\n",
        "    model.add(Dense(8, input_dim=n_states, activation='relu'))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(n_actions, activation='sigmoid'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001))\n",
        "    return model'''\n",
        "\n",
        "def policy(env, state, theta):\n",
        "    \"\"\" TODO: return probabilities for actions under softmax action selection \"\"\"\n",
        "    h = np.matmul(state, theta).flatten() # Action preference function\n",
        "    p = np.exp(h) \n",
        "    total = np.sum(p)\n",
        "    policy = p/total\n",
        "    return policy\n",
        "\n",
        "def discounted_rewards(rewards):\n",
        "    G = np.zeros_like(rewards)\n",
        "    future_rewards = 0\n",
        "    for i in reversed(range(0, len(rewards))):\n",
        "        future_rewards = gamma*future_rewards + rewards[i]\n",
        "        G[i] = future_rewards                # G_t = R_(t+1) + γ * G_(t+1)\n",
        "    return G\n",
        "\n",
        "\n",
        "def gradient(state, theta):\n",
        "    h = np.matmul(state, theta)\n",
        "    p = np.exp(h)\n",
        "    prob = p / np.sum(p)\n",
        "    grad = state - (prob[0] * state + prob[1] * state)  # Check David Silver's slide PolicyGradient : Slide 17\n",
        "    \n",
        "    return grad\n",
        "\n",
        "def generate_episode(env, theta, display=False):\n",
        "    \"\"\" Generates one episode and returns the list of states, the list of rewards and the list of actions of that episode \"\"\"\n",
        "    state = env.reset()\n",
        "    states = [state]\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    for t in range(500):\n",
        "        if display:\n",
        "            env.render()\n",
        "        p = policy(env, state, theta)\n",
        "        action = np.random.choice(len(p), p=p)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        rewards.append(reward)\n",
        "        actions.append(action)\n",
        "        if done:\n",
        "            break\n",
        "        states.append(state)\n",
        "\n",
        "    return states, rewards, actions\n",
        "\n",
        "\n",
        "def REINFORCE(env):\n",
        "    theta = np.random.rand(4, 2)  # policy parameters\n",
        "    alpha = 0.1\n",
        "    episode_lengths = []\n",
        "    length_per_hundred = []\n",
        "    reward_sum = []\n",
        "    reward_per_hundred = []\n",
        "\n",
        "    for e in range(EPISODES):\n",
        "        if e % 100 == 0:\n",
        "            length_per_hundred.append(np.mean(episode_lengths))\n",
        "            reward_per_hundred.append(reward_sum)\n",
        "            episode_lengths = []\n",
        "            reward_sum = []\n",
        "            states, rewards, actions = generate_episode(env, theta, False)  # display the policy every 300 episodes\n",
        "        else:\n",
        "            states, rewards, actions = generate_episode(env, theta, False)\n",
        "\n",
        "        T = len(states)\n",
        "\n",
        "        if e%1000==0:\n",
        "          print(\"episode: \" + str(e) + \" length: \" + str(T))\n",
        "\n",
        "        v = discounted_rewards(rewards)\n",
        "\n",
        "        # TODO: keep track of previous 100 episode lengths and compute mean\n",
        "        episode_lengths.append(T)\n",
        "        reward_sum.append(np.sum(rewards))\n",
        "\n",
        "        # TODO: implement the reinforce algorithm to improve the policy weights\n",
        "        for t in range(T-1):\n",
        "            state = states[t]\n",
        "            grad = gradient(state, theta)\n",
        "            a = actions[t]\n",
        "            theta += alpha * (gamma ** t) * grad[a] * v[t]\n",
        "\n",
        "    plt.plot(range(len(length_per_hundred)), length_per_hundred,label=\"Episode Length\")\n",
        "    \n",
        "    plt.xlabel(\"Every hundreth episode\")\n",
        "    plt.ylabel(\"Average episode length/reward\")\n",
        "    plt.title(\"Gamma = {}\".format(gamma))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    #plt.plot(range(len(reward_per_hundred)), reward_per_hundred, label=\"Reward\")\n",
        "    #plt.show()\n",
        "\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "gamma = 0.5\n",
        "REINFORCE(env)\n",
        "env.close()\n",
        "# for i in range(1,10):\n",
        "#   gamma = (i/10.0)\n",
        "#   REINFORCE(env)\n",
        "#   env.close()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode: 0 length: 37\n",
            "episode: 1000 length: 28\n",
            "episode: 2000 length: 41\n",
            "episode: 3000 length: 33\n",
            "episode: 4000 length: 71\n",
            "episode: 5000 length: 36\n",
            "episode: 6000 length: 22\n",
            "episode: 7000 length: 12\n",
            "episode: 8000 length: 14\n",
            "episode: 9000 length: 12\n",
            "episode: 10000 length: 28\n",
            "episode: 11000 length: 21\n",
            "episode: 12000 length: 29\n",
            "episode: 13000 length: 56\n",
            "episode: 14000 length: 21\n",
            "episode: 15000 length: 22\n",
            "episode: 16000 length: 26\n",
            "episode: 17000 length: 14\n",
            "episode: 18000 length: 27\n",
            "episode: 19000 length: 13\n",
            "episode: 20000 length: 39\n",
            "episode: 21000 length: 19\n",
            "episode: 22000 length: 26\n",
            "episode: 23000 length: 12\n",
            "episode: 24000 length: 17\n",
            "episode: 25000 length: 26\n",
            "episode: 26000 length: 26\n",
            "episode: 27000 length: 39\n",
            "episode: 28000 length: 14\n",
            "episode: 29000 length: 18\n",
            "episode: 30000 length: 30\n",
            "episode: 31000 length: 12\n",
            "episode: 32000 length: 24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9e1e2e886e15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m \u001b[0mREINFORCE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;31m# for i in range(1,10):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-9e1e2e886e15>\u001b[0m in \u001b[0;36mREINFORCE\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# display the policy every 300 episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-9e1e2e886e15>\u001b[0m in \u001b[0;36mgenerate_episode\u001b[0;34m(env, theta, display)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-9e1e2e886e15>\u001b[0m in \u001b[0;36mpolicy\u001b[0;34m(env, state, theta)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;34m\"\"\" TODO: return probabilities for actions under softmax action selection \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Action preference function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV3j4_TxlDb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient(state, theta, action):\n",
        "    # Using Finite differences\n",
        "    h = np.dot(state, theta)\n",
        "    p = np.exp(h)\n",
        "    prob = p / np.sum(p)\n",
        "    grad = state - (prob[0] * state + prob[1] * state)  # Check David Silver's slide PolicyGradient : Slide 17\n",
        "\n",
        "    return grad[action]\n",
        "\n",
        "\n",
        "    def gradient(state, theta):\n",
        "    # Using Finite differences\n",
        "    h = 0.0001\n",
        "    theta1 = theta + h\n",
        "    f0 = np.log(np.exp(np.dot(state, theta)))\n",
        "    f1 = np.log(np.exp(np.dot(state, theta1)))\n",
        "    grad = (f1 - f0)/h\n",
        "\n",
        "    return grad"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Option-Critic.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMD6xBPGsmW2AxbYmzjcEXj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoumyadeepB/Reinforcement-Learning/blob/master/Option_Critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tltGOf3DmEwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMPORT FourRooms\n",
        "import numpy as np\n",
        "\n",
        "class FourRooms:\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tlayout = \"\"\"\\\n",
        "wwwwwwwwwwwww\n",
        "w     w     w\n",
        "w     w     w\n",
        "w           w\n",
        "w     w     w\n",
        "w     w     w\n",
        "ww wwww     w\n",
        "w     www www\n",
        "w     w     w\n",
        "w     w     w\n",
        "w           w\n",
        "w     w     w\n",
        "wwwwwwwwwwwww\n",
        "\"\"\"\n",
        "\t\tself.occupancy = np.array([list(map(lambda c: 1 if c=='w' else 0, line)) for line in layout.splitlines()])\n",
        "\t\t\n",
        "\t\t# Four possible actions\n",
        "\t\t# 0: UP\n",
        "\t\t# 1: DOWN\n",
        "\t\t# 2: LEFT\n",
        "\t\t# 3: RIGHT\n",
        "\t\tself.action_space = np.array([0, 1, 2, 3])\n",
        "\t\tself.observation_space = np.zeros(np.sum(self.occupancy == 0))\n",
        "\t\tself.directions = [np.array((-1,0)), np.array((1,0)), np.array((0,-1)), np.array((0,1))]\n",
        "\n",
        "\t\t# Random number generator\n",
        "\t\tself.rng = np.random.RandomState(1234)\n",
        "\n",
        "\t\tself.tostate = {}\n",
        "\t\tstatenum = 0\n",
        "\t\tfor i in range(13):\n",
        "\t\t\tfor j in range(13):\n",
        "\t\t\t\tif self.occupancy[i,j] == 0:\n",
        "\t\t\t\t\tself.tostate[(i,j)] = statenum\n",
        "\t\t\t\t\tstatenum += 1\n",
        "\t\tself.tocell = {v:k for k, v in self.tostate.items()}\n",
        "\n",
        "\n",
        "\t\tself.goal = 62 # East doorway\n",
        "\t\tself.init_states = list(range(self.observation_space.shape[0]))\n",
        "\t\tself.init_states.remove(self.goal)\n",
        "\n",
        "\n",
        "\tdef render(self, show_goal=True):\n",
        "\t\tcurrent_grid = np.array(self.occupancy)\n",
        "\t\tcurrent_grid[self.current_cell[0], self.current_cell[1]] = -1\n",
        "\t\tif show_goal:\n",
        "\t\t\tgoal_cell = self.tocell[self.goal]\n",
        "\t\t\tcurrent_grid[goal_cell[0], goal_cell[1]] = -1\n",
        "\t\treturn current_grid\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\tstate = self.rng.choice(self.init_states)\n",
        "\t\tself.current_cell = self.tocell[state]\n",
        "\t\treturn state\n",
        "\n",
        "\tdef check_available_cells(self, cell):\n",
        "\t\tavailable_cells = []\n",
        "\n",
        "\t\tfor action in range(len(self.action_space)):\n",
        "\t\t\tnext_cell = tuple(cell + self.directions[action])\n",
        "\n",
        "\t\t\tif not self.occupancy[next_cell]:\n",
        "\t\t\t\tavailable_cells.append(next_cell)\n",
        "\n",
        "\t\treturn available_cells\n",
        "\t\t\n",
        "\n",
        "\tdef step(self, action):\n",
        "\t\t'''\n",
        "\t\tTakes a step in the environment with 2/3 probability. And takes a step in the\n",
        "\t\tother directions with probability 1/3 with all of them being equally likely.\n",
        "\t\t'''\n",
        "\n",
        "\t\tnext_cell = tuple(self.current_cell + self.directions[action])\n",
        "\n",
        "\t\tif not self.occupancy[next_cell]:\n",
        "\n",
        "\t\t\tif self.rng.uniform() < 1/3:\n",
        "\t\t\t\tavailable_cells = self.check_available_cells(self.current_cell)\n",
        "\t\t\t\tself.current_cell = available_cells[self.rng.randint(len(available_cells))]\n",
        "\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.current_cell = next_cell\n",
        "\n",
        "\t\tstate = self.tostate[self.current_cell]\n",
        "\n",
        "\t\t# When goal is reached, it is done\n",
        "\t\tdone = state == self.goal\n",
        "\n",
        "\n",
        "\t\treturn state, float(done), done, None"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oab1-cRzmIhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fourrooms import FourRooms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.special import logsumexp, expit\n",
        "\n",
        "\n",
        "class SigmoidTermination():\n",
        "    \"\"\" Sigmoid for option termination \"\"\"\n",
        "    def __init__(self, nstates):\n",
        "        self.nstates = nstates\n",
        "        self.weights = np.zeros((nstates,))\n",
        "\n",
        "    def pmf(self, state):\n",
        "        return expit(self.weights[state])\n",
        "\n",
        "    def sample(self, state):\n",
        "        return int(np.random.uniform() < self.pmf(state))\n",
        "\n",
        "    def gradient(self, state):\n",
        "        return self.pmf(state) * (1. - self.pmf(state))\n",
        "\n",
        "\n",
        "class SoftmaxPolicy():\n",
        "    \"\"\" Softmax policy to select intra-option primitive actions\"\"\"\n",
        "    def __init__(self, nstates, nactions, temperature=1.0):\n",
        "        self.nstates = nstates\n",
        "        self.nactions = nactions\n",
        "        self.temperature = temperature\n",
        "        self.weights = np.zeros((nstates, nactions))\n",
        "\n",
        "    def pmf(self, state):\n",
        "        exponent = self.weights[state,:] / self.temperature\n",
        "        return np.exp(exponent - logsumexp(exponent))\n",
        "\n",
        "    def sample(self, state):\n",
        "        return int(np.random.choice(self.nactions, p=self.pmf(state)))\n",
        "\n",
        "    def loggradient(self, state, action):\n",
        "        g = np.zeros((self.nstates, self.nactions))\n",
        "        g[state, :] -= self.pmf(state)\n",
        "        g[state, action] += 1\n",
        "        return g\n",
        "\n",
        "\n",
        "def policy_options(state, Q_omega, epsilon=0.1):\n",
        "    \"\"\" Epsilon-greedy policy used to select options \"\"\"\n",
        "    if np.random.uniform() < epsilon:\n",
        "        return np.random.choice(range(Q_omega.shape[1]))\n",
        "    else:\n",
        "        return np.argmax(Q_omega[state])\n",
        "\n",
        "\n",
        "def plot_termination_maps(env, terminations):\n",
        "    \"\"\" Helper function for visualization of option terminations \"\"\"\n",
        "    termination_maps = [env.occupancy.astype('float64') for _ in range(len(terminations))]\n",
        "    for option in range(len(terminations)):\n",
        "        state = 0\n",
        "        for i in range(13):\n",
        "            for j in range(13):\n",
        "                if termination_maps[option][i,j] == 0:\n",
        "                    termination_maps[option][i,j] = terminations[option].pmf(state)\n",
        "                    state += 1\n",
        "\n",
        "    for o_n, t in enumerate(termination_maps):\n",
        "        plt.imshow(t, cmap='Blues')\n",
        "        plt.axis('off')\n",
        "        plt.title(\"Option \" + str(o_n) + \" Termination\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def option_critic(env):\n",
        "    noptions = 4  # number of options\n",
        "    nepisodes = 1000  # number of episodes\n",
        "    nsteps = 1000  # max number of steps per episode\n",
        "    \n",
        "    gamma = 0.99  # discount factor\n",
        "    lr_term = 0.3  # learning rate for terminations\n",
        "    lr_intra = 0.3  # learning rate for intra option policy\n",
        "    lr_critic = 0.5  # learning rate for critic\n",
        "\n",
        "    temperature = 1e-2  # softmax with temperature (Boltzmann distribution)\n",
        "\n",
        "    nstates = env.observation_space.shape[0]\n",
        "    nactions = env.action_space.shape[0]\n",
        "\n",
        "    history = np.zeros(nepisodes)\n",
        "\n",
        "    # for each option we create a softmax policy and a sigmoid termination function:\n",
        "    option_policies = [SoftmaxPolicy(nstates, nactions, temperature) for _ in range(noptions)]\n",
        "    option_terminations = [SigmoidTermination(nstates) for _ in range(noptions)]\n",
        "\n",
        "    # tabular Q functions\n",
        "    Q_omega = np.zeros((nstates, noptions))\n",
        "    Q_U = np.zeros((nstates, noptions, nactions))\n",
        "\n",
        "    for episode in range(nepisodes):\n",
        "        state = env.reset()\n",
        "        option = policy_options(state, Q_omega)  # select option epsilon greedy\n",
        "\n",
        "        for step in range(nsteps):\n",
        "            action = option_policies[option].sample(state)  # select primitive action from option\n",
        "            nextstate, reward, done, _ = env.step(action)  # perform action: observe nextstate, reward\n",
        "\n",
        "            # TODO: 1. Options evaluation\n",
        "\n",
        "\n",
        "            # TODO: 2. Options improvement\n",
        "            # policies:\n",
        "            # you can access the weights using: option_policies[option].weights and the log gradient using option_policies[option].loggradient(state, action)\n",
        "            # terminations:\n",
        "            # you can access the weights using: option_terminations[option].weights and the gradient using option_terminations[option].gradient(state)\n",
        "            \n",
        "            # when option terminates we select a new option\n",
        "            if option_terminations[option].sample(nextstate):\n",
        "                option = policy_options(nextstate, Q_omega)\n",
        "\n",
        "            state = nextstate  # update state\n",
        "\n",
        "            if done:\n",
        "                break  # episode ends\n",
        "\n",
        "        history[episode] = step\n",
        "        print (episode, step)\n",
        "\n",
        "    plt.plot(history)\n",
        "    plt.show()\n",
        "    plot_termination_maps(env, option_terminations)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    env = FourRooms()\n",
        "    state = env.reset()\n",
        "    print(state)\n",
        "    option_critic(env)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}